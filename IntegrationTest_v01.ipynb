{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 15:42:24 DEBUG gensim.models.doc2vec: Fast version of gensim.models.doc2vec is being used\n",
      "31-Jan-2017 15:42:24 INFO summa.preprocessing.cleaner: 'pattern' package not found; tag filters are not available for English\n",
      "31-Jan-2017 15:42:24 INFO root: Generating grammar tables from /usr/lib/python3.5/lib2to3/Grammar.txt\n",
      "31-Jan-2017 15:42:24 INFO root: Generating grammar tables from /usr/lib/python3.5/lib2to3/PatternGrammar.txt\n",
      "/usr/local/lib/python3.5/dist-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/usr/local/lib/python3.5/dist-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/usr/local/lib/python3.5/dist-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/usr/local/lib/python3.5/dist-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/usr/local/lib/python3.5/dist-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/usr/local/lib/python3.5/dist-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n"
     ]
    }
   ],
   "source": [
    "from dariah_topics import preprocessing as pre\n",
    "from dariah_topics import visualization as visual\n",
    "from dariah_topics import mallet as mal\n",
    "# Warning is Gensim related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liste mit Dateinamen erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 14:49:17 INFO preprocessing: Creating document list from TXT files ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: 17 entries in document list.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpus_txt/Poe_EurekaAProsePoem.txt',\n",
       " 'corpus_txt/Howard_TheDevilinIron.txt',\n",
       " 'corpus_txt/Lovecraft_TheShunnedHouse.txt',\n",
       " 'corpus_txt/Howard_SchadowsinZamboula.txt',\n",
       " 'corpus_txt/Doyle_AStudyinScarlet.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_txt = \"corpus_txt\"\n",
    "#path_txt = \"grenzbote_plain/*/\"\n",
    "\n",
    "doclist_txt = pre.create_document_list(path_txt)\n",
    "assert doclist_txt, \"No documents found\"\n",
    "doclist_txt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 14:49:17 INFO preprocessing: Creating document list from CSV files ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: 16 entries in document list.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpus_csv/Howard_GodsoftheNorth.txt.csv',\n",
       " 'corpus_csv/Poe_EurekaAProsePoem.txt.csv',\n",
       " 'corpus_csv/Poe_TheMasqueoftheRedDeath.txt.csv',\n",
       " 'corpus_csv/Poe_ThePurloinedLetter.txt.csv',\n",
       " 'corpus_csv/Howard_ShadowsintheMoonlight.txt.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_csv = \"corpus_csv\"\n",
    "\n",
    "doclist_csv = pre.create_document_list(path_csv, 'csv')\n",
    "doclist_csv[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Liste mit Dokumentenlabels erzeugen - (Funktion wird durch Thorsten's generischere Funktion ersetzt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 14:49:17 INFO preprocessing: Creating document labels ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Document labels available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpus_txt/Poe_EurekaAProsePoem.txt',\n",
       " 'corpus_txt/Howard_TheDevilinIron.txt',\n",
       " 'corpus_txt/Lovecraft_TheShunnedHouse.txt',\n",
       " 'corpus_txt/Howard_SchadowsinZamboula.txt',\n",
       " 'corpus_txt/Doyle_AStudyinScarlet.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_labels = list(pre.get_labels(doclist_txt))\n",
    "doc_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_txt = pre.read_from_txt(doclist_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_csv = pre.read_from_csv(doclist_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Poe_EurekaAProsePoem.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Howard_TheDevilinIron.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Lovecraft_TheShunnedHouse.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Howard_SchadowsinZamboula.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Doyle_AStudyinScarlet.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Poe_TheCaskofAmontillado.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Poe_TheMasqueoftheRedDeath.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Howard_GodsoftheNorth.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Kipling_TheEndofthePassage.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Doyle_TheSignoftheFour.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Kipling_TheJungleBook.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Doyle_AScandalinBohemia.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Poe_ThePurloinedLetter.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Lovecraft_AttheMountainofMadness.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Kipling_ThyServantaDog.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Howard_ShadowsintheMoonlight.txt ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: Accessing TXT document corpus_txt/Doyle_TheHoundoftheBaskervilles.txt ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['eureka',\n",
       "  'a',\n",
       "  'prose',\n",
       "  'poem',\n",
       "  'by',\n",
       "  'edgar',\n",
       "  'a',\n",
       "  'poe',\n",
       "  'new-york',\n",
       "  'geo',\n",
       "  'p',\n",
       "  'putnam',\n",
       "  'of',\n",
       "  'late',\n",
       "  'firm',\n",
       "  'of',\n",
       "  'wiley',\n",
       "  'putnam',\n",
       "  'broadway',\n",
       "  'mdcccxlviii',\n",
       "  'entered',\n",
       "  'according',\n",
       "  'to',\n",
       "  'act',\n",
       "  'of',\n",
       "  'congress',\n",
       "  'in',\n",
       "  'the',\n",
       "  'year',\n",
       "  'by',\n",
       "  'edgar',\n",
       "  'a',\n",
       "  'poe',\n",
       "  'in',\n",
       "  'the',\n",
       "  'clerk’s',\n",
       "  'office',\n",
       "  'of',\n",
       "  'the',\n",
       "  'district',\n",
       "  'court',\n",
       "  'for',\n",
       "  'the',\n",
       "  'southern',\n",
       "  'district',\n",
       "  'of',\n",
       "  'new-york',\n",
       "  'leavitt',\n",
       "  'trow',\n",
       "  'co',\n",
       "  'prs',\n",
       "  'ann-street',\n",
       "  'with',\n",
       "  'very',\n",
       "  'profound',\n",
       "  'respect',\n",
       "  'this',\n",
       "  'work',\n",
       "  'is',\n",
       "  'dedicated',\n",
       "  'to',\n",
       "  'alexander',\n",
       "  'von',\n",
       "  'humboldt',\n",
       "  'preface',\n",
       "  'to',\n",
       "  'the',\n",
       "  'few',\n",
       "  'who',\n",
       "  'love',\n",
       "  'me',\n",
       "  'and',\n",
       "  'whom',\n",
       "  'i',\n",
       "  'love—to',\n",
       "  'those',\n",
       "  'who',\n",
       "  'feel',\n",
       "  'rather',\n",
       "  'than',\n",
       "  'to',\n",
       "  'those',\n",
       "  'who',\n",
       "  'think—to',\n",
       "  'the',\n",
       "  'dreamers',\n",
       "  'and',\n",
       "  'those',\n",
       "  'who',\n",
       "  'put',\n",
       "  'faith',\n",
       "  'in',\n",
       "  'dreams',\n",
       "  'as',\n",
       "  'in',\n",
       "  'the',\n",
       "  'only',\n",
       "  'realities—i',\n",
       "  'offer',\n",
       "  'this',\n",
       "  'book',\n",
       "  'of',\n",
       "  'truths',\n",
       "  'not',\n",
       "  'in',\n",
       "  'its',\n",
       "  'character',\n",
       "  'of',\n",
       "  'truth-teller',\n",
       "  'but',\n",
       "  'for',\n",
       "  'the',\n",
       "  'beauty',\n",
       "  'that',\n",
       "  'abounds',\n",
       "  'in',\n",
       "  'its',\n",
       "  'truth',\n",
       "  'constituting',\n",
       "  'it',\n",
       "  'true',\n",
       "  'to',\n",
       "  'these',\n",
       "  'i',\n",
       "  'present',\n",
       "  'the',\n",
       "  'composition',\n",
       "  'as',\n",
       "  'an',\n",
       "  'art-product',\n",
       "  'alone:—let',\n",
       "  'us',\n",
       "  'say',\n",
       "  'as',\n",
       "  'a',\n",
       "  'romance',\n",
       "  'or',\n",
       "  'if',\n",
       "  'i',\n",
       "  'be',\n",
       "  'not',\n",
       "  'urging',\n",
       "  'too',\n",
       "  'lofty',\n",
       "  'a',\n",
       "  'claim',\n",
       "  'as',\n",
       "  'a',\n",
       "  'poem',\n",
       "  'what',\n",
       "  'i',\n",
       "  'here',\n",
       "  'propound',\n",
       "  'is',\n",
       "  'true_:—therefore',\n",
       "  'it',\n",
       "  'cannot',\n",
       "  'die:—or',\n",
       "  'if',\n",
       "  'by',\n",
       "  'any',\n",
       "  'means',\n",
       "  'it',\n",
       "  'be',\n",
       "  'now',\n",
       "  'trodden',\n",
       "  'down',\n",
       "  'so',\n",
       "  'that',\n",
       "  'it',\n",
       "  'die',\n",
       "  'it',\n",
       "  'will',\n",
       "  'rise',\n",
       "  'again',\n",
       "  'to',\n",
       "  'the',\n",
       "  'life',\n",
       "  'everlasting',\n",
       "  'nevertheless',\n",
       "  'it',\n",
       "  'is',\n",
       "  'as',\n",
       "  'a',\n",
       "  'poem',\n",
       "  'only',\n",
       "  'that',\n",
       "  'i',\n",
       "  'wish',\n",
       "  'this',\n",
       "  'work',\n",
       "  'to',\n",
       "  'be',\n",
       "  'judged',\n",
       "  'after',\n",
       "  'i',\n",
       "  'am',\n",
       "  'dead',\n",
       "  'e',\n",
       "  'a',\n",
       "  'p',\n",
       "  'eureka',\n",
       "  'an',\n",
       "  'essay',\n",
       "  'on',\n",
       "  'the',\n",
       "  'material',\n",
       "  'and',\n",
       "  'spiritual',\n",
       "  'universe',\n",
       "  'it',\n",
       "  'is',\n",
       "  'with',\n",
       "  'humility',\n",
       "  'really',\n",
       "  'unassumed—it',\n",
       "  'is',\n",
       "  'with',\n",
       "  'a',\n",
       "  'sentiment',\n",
       "  'even',\n",
       "  'of',\n",
       "  'awe—that',\n",
       "  'i',\n",
       "  'pen',\n",
       "  'the',\n",
       "  'opening',\n",
       "  'sentence',\n",
       "  'of',\n",
       "  'this',\n",
       "  'work',\n",
       "  'for',\n",
       "  'of',\n",
       "  'all',\n",
       "  'conceivable',\n",
       "  'subjects',\n",
       "  'i',\n",
       "  'approach',\n",
       "  'the',\n",
       "  'reader',\n",
       "  'with',\n",
       "  'the',\n",
       "  'most',\n",
       "  'solemn—the',\n",
       "  'most',\n",
       "  'comprehensive—the',\n",
       "  'most',\n",
       "  'difficult—the',\n",
       "  'most',\n",
       "  'august',\n",
       "  'what',\n",
       "  'terms',\n",
       "  'shall',\n",
       "  'i',\n",
       "  'find',\n",
       "  'sufficiently',\n",
       "  'simple',\n",
       "  'in',\n",
       "  'their',\n",
       "  'sublimity—sufficiently',\n",
       "  'sublime',\n",
       "  'in',\n",
       "  'their',\n",
       "  'simplicity—for',\n",
       "  'the',\n",
       "  'mere',\n",
       "  'enunciation',\n",
       "  'of',\n",
       "  'my',\n",
       "  'theme',\n",
       "  'i',\n",
       "  'design',\n",
       "  'to',\n",
       "  'speak',\n",
       "  'of',\n",
       "  'the',\n",
       "  'physical',\n",
       "  'metaphysical',\n",
       "  'and',\n",
       "  'mathematical—of',\n",
       "  'the',\n",
       "  'material',\n",
       "  'and',\n",
       "  'spiritual',\n",
       "  'universe:—of',\n",
       "  'its',\n",
       "  'essence',\n",
       "  'its',\n",
       "  'origin',\n",
       "  'its',\n",
       "  'creation',\n",
       "  'its',\n",
       "  'present',\n",
       "  'condition',\n",
       "  'and',\n",
       "  'its',\n",
       "  'destiny',\n",
       "  'i',\n",
       "  'shall',\n",
       "  'be',\n",
       "  'so',\n",
       "  'rash',\n",
       "  'moreover',\n",
       "  'as',\n",
       "  'to',\n",
       "  'challenge',\n",
       "  'the',\n",
       "  'conclusions',\n",
       "  'and',\n",
       "  'thus',\n",
       "  'in',\n",
       "  'effect',\n",
       "  'to',\n",
       "  'question',\n",
       "  'the',\n",
       "  'sagacity',\n",
       "  'of',\n",
       "  'many',\n",
       "  'of',\n",
       "  'the',\n",
       "  'greatest',\n",
       "  'and',\n",
       "  'most',\n",
       "  'justly',\n",
       "  'reverenced',\n",
       "  'of',\n",
       "  'men',\n",
       "  'in',\n",
       "  'the',\n",
       "  'beginning',\n",
       "  'let',\n",
       "  'me',\n",
       "  'as',\n",
       "  'distinctly',\n",
       "  'as',\n",
       "  'possible',\n",
       "  'announce—not',\n",
       "  'the',\n",
       "  'theorem',\n",
       "  'which',\n",
       "  'i',\n",
       "  'hope',\n",
       "  'to',\n",
       "  'demonstrate—for',\n",
       "  'whatever',\n",
       "  'the',\n",
       "  'mathematicians',\n",
       "  'may',\n",
       "  'assert',\n",
       "  'there',\n",
       "  'is',\n",
       "  'in',\n",
       "  'this',\n",
       "  'world',\n",
       "  'at',\n",
       "  'least',\n",
       "  'no',\n",
       "  'such',\n",
       "  'thing',\n",
       "  'as',\n",
       "  'demonstration—but',\n",
       "  'the',\n",
       "  'ruling',\n",
       "  'idea',\n",
       "  'which',\n",
       "  'throughout',\n",
       "  'this',\n",
       "  'volume',\n",
       "  'i',\n",
       "  'shall',\n",
       "  'be',\n",
       "  'continually',\n",
       "  'endeavoring',\n",
       "  'to',\n",
       "  'suggest',\n",
       "  'my',\n",
       "  'general',\n",
       "  'proposition',\n",
       "  'then',\n",
       "  'is',\n",
       "  'this:—_in',\n",
       "  'the',\n",
       "  'original',\n",
       "  'unity',\n",
       "  'of',\n",
       "  'the',\n",
       "  'first',\n",
       "  'thing',\n",
       "  'lies',\n",
       "  'the',\n",
       "  'secondary',\n",
       "  'cause',\n",
       "  'of',\n",
       "  'all',\n",
       "  'things',\n",
       "  'with',\n",
       "  'the',\n",
       "  'germ',\n",
       "  'of',\n",
       "  'their',\n",
       "  'inevitable',\n",
       "  'annihilation',\n",
       "  'in',\n",
       "  'illustration',\n",
       "  'of',\n",
       "  'this',\n",
       "  'idea',\n",
       "  'i',\n",
       "  'propose',\n",
       "  'to',\n",
       "  'take',\n",
       "  'such',\n",
       "  'a',\n",
       "  'survey',\n",
       "  'of',\n",
       "  'the',\n",
       "  'universe',\n",
       "  'that',\n",
       "  'the',\n",
       "  'mind',\n",
       "  'may',\n",
       "  'be',\n",
       "  'able',\n",
       "  'really',\n",
       "  'to',\n",
       "  'receive',\n",
       "  'and',\n",
       "  'to',\n",
       "  'perceive',\n",
       "  'an',\n",
       "  'individual',\n",
       "  'impression',\n",
       "  'he',\n",
       "  'who',\n",
       "  'from',\n",
       "  'the',\n",
       "  'top',\n",
       "  'of',\n",
       "  'ætna',\n",
       "  'casts',\n",
       "  'his',\n",
       "  'eyes',\n",
       "  'leisurely',\n",
       "  'around',\n",
       "  'is',\n",
       "  'affected',\n",
       "  'chiefly',\n",
       "  'by',\n",
       "  'the',\n",
       "  'extent',\n",
       "  'and',\n",
       "  'diversity',\n",
       "  'of',\n",
       "  'the',\n",
       "  'scene',\n",
       "  'only',\n",
       "  'by',\n",
       "  'a',\n",
       "  'rapid',\n",
       "  'whirling',\n",
       "  'on',\n",
       "  'his',\n",
       "  'heel',\n",
       "  'could',\n",
       "  'he',\n",
       "  'hope',\n",
       "  'to',\n",
       "  'comprehend',\n",
       "  'the',\n",
       "  'panorama',\n",
       "  'in',\n",
       "  'the',\n",
       "  'sublimity',\n",
       "  'of',\n",
       "  'its',\n",
       "  'oneness',\n",
       "  'but',\n",
       "  'as',\n",
       "  'on',\n",
       "  'the',\n",
       "  'summit',\n",
       "  'of',\n",
       "  'ætna',\n",
       "  'no',\n",
       "  'man',\n",
       "  'has',\n",
       "  'thought',\n",
       "  'of',\n",
       "  'whirling',\n",
       "  'on',\n",
       "  'his',\n",
       "  'heel',\n",
       "  'so',\n",
       "  'no',\n",
       "  'man',\n",
       "  'has',\n",
       "  'ever',\n",
       "  'taken',\n",
       "  'into',\n",
       "  'his',\n",
       "  'brain',\n",
       "  'the',\n",
       "  'full',\n",
       "  'uniqueness',\n",
       "  'of',\n",
       "  'the',\n",
       "  'prospect',\n",
       "  'and',\n",
       "  'so',\n",
       "  'again',\n",
       "  'whatever',\n",
       "  'considerations',\n",
       "  'lie',\n",
       "  'involved',\n",
       "  'in',\n",
       "  'this',\n",
       "  'uniqueness',\n",
       "  'have',\n",
       "  'as',\n",
       "  'yet',\n",
       "  'no',\n",
       "  'practical',\n",
       "  'existence',\n",
       "  'for',\n",
       "  'mankind',\n",
       "  'i',\n",
       "  'do',\n",
       "  'not',\n",
       "  'know',\n",
       "  'a',\n",
       "  'treatise',\n",
       "  'in',\n",
       "  'which',\n",
       "  'a',\n",
       "  'survey',\n",
       "  'of',\n",
       "  'the',\n",
       "  'universe_—using',\n",
       "  'the',\n",
       "  'word',\n",
       "  'in',\n",
       "  'its',\n",
       "  'most',\n",
       "  'comprehensive',\n",
       "  'and',\n",
       "  'only',\n",
       "  'legitimate',\n",
       "  'acceptation—is',\n",
       "  'taken',\n",
       "  'at',\n",
       "  'all:—and',\n",
       "  'it',\n",
       "  'may',\n",
       "  'be',\n",
       "  'as',\n",
       "  'well',\n",
       "  'here',\n",
       "  'to',\n",
       "  'mention',\n",
       "  'that',\n",
       "  'by',\n",
       "  'the',\n",
       "  'term',\n",
       "  'universe',\n",
       "  'wherever',\n",
       "  'employed',\n",
       "  'without',\n",
       "  'qualification',\n",
       "  'in',\n",
       "  'this',\n",
       "  'essay',\n",
       "  'i',\n",
       "  'mean',\n",
       "  'to',\n",
       "  'designate',\n",
       "  'the',\n",
       "  'utmost',\n",
       "  'conceivable',\n",
       "  'expanse',\n",
       "  'of',\n",
       "  'space',\n",
       "  'with',\n",
       "  'all',\n",
       "  'things',\n",
       "  'spiritual',\n",
       "  'and',\n",
       "  'material',\n",
       "  'that',\n",
       "  'can',\n",
       "  'be',\n",
       "  'imagined',\n",
       "  'to',\n",
       "  'exist',\n",
       "  'within',\n",
       "  'the',\n",
       "  'compass',\n",
       "  'of',\n",
       "  'that',\n",
       "  'expanse',\n",
       "  'in',\n",
       "  'speaking',\n",
       "  'of',\n",
       "  'what',\n",
       "  'is',\n",
       "  'ordinarily',\n",
       "  'implied',\n",
       "  'by',\n",
       "  'the',\n",
       "  'expression',\n",
       "  'universe',\n",
       "  'i',\n",
       "  'shall',\n",
       "  'take',\n",
       "  'a',\n",
       "  'phrase',\n",
       "  'of',\n",
       "  'limitation—“the',\n",
       "  'universe',\n",
       "  'of',\n",
       "  'stars',\n",
       "  'why',\n",
       "  'this',\n",
       "  'distinction',\n",
       "  'is',\n",
       "  'considered',\n",
       "  'necessary',\n",
       "  'will',\n",
       "  'be',\n",
       "  'seen',\n",
       "  'in',\n",
       "  'the',\n",
       "  'sequel',\n",
       "  'but',\n",
       "  'even',\n",
       "  'of',\n",
       "  'treatises',\n",
       "  'on',\n",
       "  'the',\n",
       "  'really',\n",
       "  'limited',\n",
       "  'although',\n",
       "  'always',\n",
       "  'assumed',\n",
       "  'as',\n",
       "  'the',\n",
       "  'un_limited',\n",
       "  'universe',\n",
       "  'of',\n",
       "  'stars',\n",
       "  'i',\n",
       "  'know',\n",
       "  'none',\n",
       "  'in',\n",
       "  'which',\n",
       "  'a',\n",
       "  'survey',\n",
       "  'even',\n",
       "  'of',\n",
       "  'this',\n",
       "  'limited',\n",
       "  'universe',\n",
       "  'is',\n",
       "  'so',\n",
       "  'taken',\n",
       "  'as',\n",
       "  'to',\n",
       "  'warrant',\n",
       "  'deductions',\n",
       "  'from',\n",
       "  'its',\n",
       "  'individuality',\n",
       "  'the',\n",
       "  'nearest',\n",
       "  'approach',\n",
       "  'to',\n",
       "  'such',\n",
       "  'a',\n",
       "  'work',\n",
       "  'is',\n",
       "  'made',\n",
       "  'in',\n",
       "  'the',\n",
       "  'cosmos',\n",
       "  'of',\n",
       "  'alexander',\n",
       "  'von',\n",
       "  'humboldt',\n",
       "  'he',\n",
       "  'presents',\n",
       "  'the',\n",
       "  'subject',\n",
       "  'however',\n",
       "  'not',\n",
       "  'in',\n",
       "  'its',\n",
       "  'individuality',\n",
       "  'but',\n",
       "  'in',\n",
       "  'its',\n",
       "  'generality',\n",
       "  'his',\n",
       "  'theme',\n",
       "  'in',\n",
       "  'its',\n",
       "  'last',\n",
       "  'result',\n",
       "  'is',\n",
       "  'the',\n",
       "  'law',\n",
       "  'of',\n",
       "  'each',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'the',\n",
       "  'merely',\n",
       "  'physical',\n",
       "  'universe',\n",
       "  'as',\n",
       "  'this',\n",
       "  'law',\n",
       "  'is',\n",
       "  'related',\n",
       "  'to',\n",
       "  'the',\n",
       "  'laws',\n",
       "  'of',\n",
       "  'every',\n",
       "  'other',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'this',\n",
       "  'merely',\n",
       "  'physical',\n",
       "  'universe',\n",
       "  'his',\n",
       "  'design',\n",
       "  'is',\n",
       "  'simply',\n",
       "  'synœretical',\n",
       "  'in',\n",
       "  'a',\n",
       "  'word',\n",
       "  'he',\n",
       "  'discusses',\n",
       "  'the',\n",
       "  'universality',\n",
       "  'of',\n",
       "  'material',\n",
       "  'relation',\n",
       "  'and',\n",
       "  'discloses',\n",
       "  'to',\n",
       "  'the',\n",
       "  'eye',\n",
       "  'of',\n",
       "  'philosophy',\n",
       "  'whatever',\n",
       "  'inferences',\n",
       "  'have',\n",
       "  'hitherto',\n",
       "  'lain',\n",
       "  'hidden',\n",
       "  'behind',\n",
       "  'this',\n",
       "  'universality',\n",
       "  'but',\n",
       "  'however',\n",
       "  'admirable',\n",
       "  'be',\n",
       "  'the',\n",
       "  'succinctness',\n",
       "  'with',\n",
       "  'which',\n",
       "  'he',\n",
       "  'has',\n",
       "  'treated',\n",
       "  'each',\n",
       "  'particular',\n",
       "  'point',\n",
       "  'of',\n",
       "  'his',\n",
       "  'topic',\n",
       "  'the',\n",
       "  'mere',\n",
       "  'multiplicity',\n",
       "  'of',\n",
       "  'these',\n",
       "  'points',\n",
       "  'occasions',\n",
       "  'necessarily',\n",
       "  'an',\n",
       "  'amount',\n",
       "  'of',\n",
       "  'detail',\n",
       "  'and',\n",
       "  'thus',\n",
       "  'an',\n",
       "  'involution',\n",
       "  'of',\n",
       "  'idea',\n",
       "  'which',\n",
       "  'precludes',\n",
       "  'all',\n",
       "  'individuality',\n",
       "  'of',\n",
       "  'impression',\n",
       "  'it',\n",
       "  'seems',\n",
       "  'to',\n",
       "  'me',\n",
       "  'that',\n",
       "  'in',\n",
       "  'aiming',\n",
       "  'at',\n",
       "  'this',\n",
       "  'latter',\n",
       "  'effect',\n",
       "  'and',\n",
       "  'through',\n",
       "  'it',\n",
       "  'at',\n",
       "  'the',\n",
       "  'consequences—the',\n",
       "  'conclusions—the',\n",
       "  'suggestions—the',\n",
       "  'speculations—or',\n",
       "  'if',\n",
       "  'nothing',\n",
       "  'better',\n",
       "  'offer',\n",
       "  'itself',\n",
       "  'the',\n",
       "  'mere',\n",
       "  'guesses',\n",
       "  'which',\n",
       "  'may',\n",
       "  'result',\n",
       "  'from',\n",
       "  'it—we',\n",
       "  'require',\n",
       "  'something',\n",
       "  'like',\n",
       "  'a',\n",
       "  'mental',\n",
       "  'gyration',\n",
       "  'on',\n",
       "  'the',\n",
       "  'heel',\n",
       "  'we',\n",
       "  'need',\n",
       "  'so',\n",
       "  'rapid',\n",
       "  'a',\n",
       "  'revolution',\n",
       "  'of',\n",
       "  'all',\n",
       "  'things',\n",
       "  'about',\n",
       "  'the',\n",
       "  'central',\n",
       "  'point',\n",
       "  'of',\n",
       "  'sight',\n",
       "  'that',\n",
       "  'while',\n",
       "  'the',\n",
       "  'minutiæ',\n",
       "  'vanish',\n",
       "  'altogether',\n",
       "  'even',\n",
       "  'the',\n",
       "  'more',\n",
       "  'conspicuous',\n",
       "  'objects',\n",
       "  'become',\n",
       "  'blended',\n",
       "  'into',\n",
       "  'one',\n",
       "  'among',\n",
       "  'the',\n",
       "  'vanishing',\n",
       "  'minutiæ',\n",
       "  'in',\n",
       "  'a',\n",
       "  'survey',\n",
       "  'of',\n",
       "  'this',\n",
       "  'kind',\n",
       "  'would',\n",
       "  'be',\n",
       "  'all',\n",
       "  'exclusively',\n",
       "  'terrestrial',\n",
       "  'matters',\n",
       "  'the',\n",
       "  'earth',\n",
       "  'would',\n",
       "  'be',\n",
       "  'considered',\n",
       "  'in',\n",
       "  'its',\n",
       "  'planetary',\n",
       "  'relations',\n",
       "  'alone',\n",
       "  'a',\n",
       "  'man',\n",
       "  'in',\n",
       "  'this',\n",
       "  'view',\n",
       "  'becomes',\n",
       "  'mankind',\n",
       "  'mankind',\n",
       "  'a',\n",
       "  'member',\n",
       "  'of',\n",
       "  'the',\n",
       "  'cosmical',\n",
       "  'family',\n",
       "  'of',\n",
       "  'intelligences',\n",
       "  'and',\n",
       "  'now',\n",
       "  'before',\n",
       "  'proceeding',\n",
       "  'to',\n",
       "  'our',\n",
       "  'subject',\n",
       "  'proper',\n",
       "  'let',\n",
       "  'me',\n",
       "  'beg',\n",
       "  'the',\n",
       "  'reader’s',\n",
       "  'attention',\n",
       "  'to',\n",
       "  'an',\n",
       "  'extract',\n",
       "  'or',\n",
       "  'two',\n",
       "  'from',\n",
       "  'a',\n",
       "  'somewhat',\n",
       "  'remarkable',\n",
       "  'letter',\n",
       "  'which',\n",
       "  'appears',\n",
       "  'to',\n",
       "  'have',\n",
       "  'been',\n",
       "  'found',\n",
       "  'corked',\n",
       "  'in',\n",
       "  'a',\n",
       "  'bottle',\n",
       "  'and',\n",
       "  'floating',\n",
       "  'on',\n",
       "  'the',\n",
       "  'mare',\n",
       "  'tenebrarum_—an',\n",
       "  'ocean',\n",
       "  'well',\n",
       "  'described',\n",
       "  'by',\n",
       "  'the',\n",
       "  'nubian',\n",
       "  'geographer',\n",
       "  'ptolemy',\n",
       "  'hephestion',\n",
       "  'but',\n",
       "  'little',\n",
       "  'frequented',\n",
       "  'in',\n",
       "  'modern',\n",
       "  'days',\n",
       "  'unless',\n",
       "  'by',\n",
       "  'the',\n",
       "  'transcendentalists',\n",
       "  'and',\n",
       "  'some',\n",
       "  'other',\n",
       "  'divers',\n",
       "  'for',\n",
       "  'crotchets',\n",
       "  'the',\n",
       "  'date',\n",
       "  'of',\n",
       "  'this',\n",
       "  'letter',\n",
       "  'i',\n",
       "  'confess',\n",
       "  'surprises',\n",
       "  'me',\n",
       "  'even',\n",
       "  'more',\n",
       "  'particularly',\n",
       "  'than',\n",
       "  'its',\n",
       "  'contents',\n",
       "  ...]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens = [list(pre.tokenize(txt)) for txt in list(corpus_txt)]\n",
    "doc_tokens[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_types, doc_ids = pre.create_dictionaries(doc_labels, doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse BOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparse_bow = pre.create_mm(doc_labels, doc_tokens, id_types, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">1</th>\n",
       "      <th>18658</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8193</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8194</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8195</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8197</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19116</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16394</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16395</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16400</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8210</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16404</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16405</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16410</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16412</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4101</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16418</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8234</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8235</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8236</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8237</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">16</th>\n",
       "      <th>16294</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23196</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16301</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24503</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16315</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16319</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10912</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8130</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8132</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24517</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24520</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24524</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24532</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24549</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16348</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8160</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24545</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24548</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24550</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24551</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13911</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11456</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8174</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16369</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8182</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16377</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8191</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52926 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "doc_id token_id    \n",
       "1      18658      2\n",
       "       8193       7\n",
       "       8194       2\n",
       "       8195      17\n",
       "       8197       5\n",
       "       1          2\n",
       "       19116      1\n",
       "       16394      1\n",
       "       16395      1\n",
       "       2          1\n",
       "       15         3\n",
       "       16400      1\n",
       "       8210       3\n",
       "       19         1\n",
       "       16404      2\n",
       "       16405      1\n",
       "       25         1\n",
       "       16410      1\n",
       "       16412      1\n",
       "       29         1\n",
       "       4101       1\n",
       "       6832       2\n",
       "       16418      2\n",
       "       35        10\n",
       "       38         1\n",
       "       39        14\n",
       "       8234      13\n",
       "       8235       1\n",
       "       8236      13\n",
       "       8237       1\n",
       "...              ..\n",
       "16     16294      0\n",
       "       23196      0\n",
       "       16301      0\n",
       "       24503      0\n",
       "       16315      0\n",
       "       16319      0\n",
       "       10912      0\n",
       "       8130       0\n",
       "       8132       0\n",
       "       24517      0\n",
       "       24520      0\n",
       "       24524      0\n",
       "       6818       0\n",
       "       24532      0\n",
       "       24549      0\n",
       "       16348      0\n",
       "       8160       0\n",
       "       24545      0\n",
       "       24548      0\n",
       "       2726       0\n",
       "       24550      0\n",
       "       24551      0\n",
       "       13911      0\n",
       "       11456      0\n",
       "       8174       0\n",
       "       16369      0\n",
       "       8182       0\n",
       "       16377      0\n",
       "       8188       0\n",
       "       8191       0\n",
       "\n",
       "[52926 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Doc-Topic Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\n\\ndoc2id = {value : key for key, value in doc_ids.items()}\\ntype2id = {value : key for key, value in id_types.items()}\\n\\ncols = [doc2id[key] for key in set(sparse_bow.index.get_level_values(\"doc_id\"))]\\n#idx = [type2id[key] for key in set(sparse_bow.index.get_level_values(\"token_id\"))]\\n\\nset(sparse_bow.index.get_level_values(\"token_id\"))\\n\\n#doctopic_matrix = pd.DataFrame(columns=cols, index=idx)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "\n",
    "doc2id = {value : key for key, value in doc_ids.items()}\n",
    "type2id = {value : key for key, value in id_types.items()}\n",
    "\n",
    "cols = [doc2id[key] for key in set(sparse_bow.index.get_level_values(\"doc_id\"))]\n",
    "#idx = [type2id[key] for key in set(sparse_bow.index.get_level_values(\"token_id\"))]\n",
    "\n",
    "set(sparse_bow.index.get_level_values(\"token_id\"))\n",
    "\n",
    "#doctopic_matrix = pd.DataFrame(columns=cols, index=idx)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "basepath = os.path.abspath('.')\n",
    "\n",
    "with open(os.path.join(basepath, \"tutorial_supplementals\", \"stopwords\", \"en\"), 'r', encoding = 'utf-8') as f: \n",
    "    stopword_list = f.read().split('\\n')\n",
    "    \n",
    "stopword_list = set(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 14:49:17 INFO preprocessing: Removing features ...\n",
      "31-Jan-2017 14:49:17 DEBUG preprocessing: 672 features removed.\n"
     ]
    }
   ],
   "source": [
    "sparse_df_stopwords_removed = pre.remove_features(sparse_bow, id_types, stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52926\n",
      "47660\n"
     ]
    }
   ],
   "source": [
    "print(len(sparse_bow))\n",
    "print(len(sparse_df_stopwords_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Feature Remove Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 14:49:18 INFO preprocessing: Finding stopwords ...\n",
      "31-Jan-2017 14:49:18 DEBUG preprocessing: 100 stopwords found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = pre.find_stopwords(sparse_bow, id_types, 100)\n",
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 14:49:18 INFO preprocessing: Find hapax legomena ...\n",
      "31-Jan-2017 14:49:18 DEBUG preprocessing: 19478 hapax legomena found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19478"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hapax_list = pre.find_hapax(sparse_bow, id_types)\n",
    "len(hapax_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 14:49:18 INFO preprocessing: Removing features ...\n",
      "31-Jan-2017 14:49:18 DEBUG preprocessing: 19542 features removed.\n"
     ]
    }
   ],
   "source": [
    "feature_list = set(stopword_list).union(hapax_list)\n",
    "clean_term_frequency = pre.remove_features(sparse_bow, id_types, feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5685"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_term_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving Sparse BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_docs = max(sparse_bow.index.get_level_values(\"doc_id\"))\n",
    "num_types = max(sparse_bow.index.get_level_values(\"token_id\"))\n",
    "sum_counts = sum(sparse_bow[0])\n",
    "\n",
    "header_string = str(num_docs) + \" \" + str(num_types) + \" \" + str(sum_counts) + \"\\n\"\n",
    "\n",
    "with open(\"gb_plain.mm\", 'w', encoding = \"utf-8\") as f:\n",
    "    pass\n",
    "\n",
    "with open(\"gb_plain.mm\", 'a', encoding = \"utf-8\") as f:\n",
    "    f.write(\"%%MatrixMarket matrix coordinate real general\\n\")\n",
    "    f.write(header_string)\n",
    "    sparse_bow.to_csv( f, sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 14:49:18 INFO gensim.matutils: initializing corpus reader from gb_plain.mm\n",
      "31-Jan-2017 14:49:18 INFO gensim.matutils: accepted corpus with 16 documents, 24552 features, 310670 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "mm = MmCorpus(\"gb_plain.mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in case you're only loading the corpus - build dict first\n",
    "\n",
    "#import pickle\n",
    "\n",
    "#pickle.dump( id_types, open( \"gb_plain.dictionary\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#id_types = pickle.load(open(\"gb_plain.dictionary\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mm = gensim.corpora.MmCorpus(\"gb_all.mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#id_types = pickle.load(open(\"gb_all.dictionary\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2id = {value : key for key, value in doc_ids.items()}\n",
    "type2id = {value : key for key, value in id_types.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 14:49:18 INFO gensim.models.ldamodel: using symmetric alpha at 0.01\n",
      "31-Jan-2017 14:49:18 INFO gensim.models.ldamodel: using symmetric eta at 4.0728220584042684e-05\n",
      "31-Jan-2017 14:49:18 INFO gensim.models.ldamodel: using serial LDA version on this node\n",
      "31-Jan-2017 14:49:43 INFO gensim.models.ldamodel: running online LDA training, 100 topics, 1 passes over the supplied corpus of 16 documents, updating model once every 16 documents, evaluating perplexity every 16 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "31-Jan-2017 14:49:43 WARNING gensim.models.ldamodel: too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "31-Jan-2017 14:50:09 DEBUG gensim.models.ldamodel: bound: at document #0\n",
      "31-Jan-2017 14:50:12 INFO gensim.models.ldamodel: -35.088 per-word bound, 36521290197.7 perplexity estimate based on a held-out corpus of 16 documents with 310670 words\n",
      "31-Jan-2017 14:50:12 INFO gensim.models.ldamodel: PROGRESS: pass 0, at document #16/16\n",
      "31-Jan-2017 14:50:12 DEBUG gensim.models.ldamodel: performing inference on a chunk of 16 documents\n",
      "31-Jan-2017 14:50:12 DEBUG gensim.models.ldamodel: 1/16 documents converged within 50 iterations\n",
      "31-Jan-2017 14:50:12 DEBUG gensim.models.ldamodel: updating topics\n",
      "31-Jan-2017 14:50:13 INFO gensim.models.ldamodel: topic #0 (0.010): 0.040*\"ample\" + 0.040*\"pack\" + 0.020*\"bruyère\" + 0.019*\"fallacies\" + 0.015*\"traced\" + 0.014*\"forty\" + 0.013*\"demonstrate\" + 0.010*\"imperious\" + 0.010*\"step—we\" + 0.008*\"dishonour\"\n",
      "31-Jan-2017 14:50:13 INFO gensim.models.ldamodel: topic #66 (0.010): 0.034*\"ample\" + 0.032*\"bruyère\" + 0.018*\"fallacies\" + 0.017*\"pack\" + 0.016*\"imperious\" + 0.014*\"traced\" + 0.013*\"dishonour\" + 0.012*\"demonstrate\" + 0.011*\"step—we\" + 0.009*\"forfeit\"\n",
      "31-Jan-2017 14:50:13 INFO gensim.models.ldamodel: topic #38 (0.010): 0.034*\"ample\" + 0.034*\"bruyère\" + 0.033*\"pack\" + 0.015*\"traced\" + 0.013*\"fallacies\" + 0.013*\"step—we\" + 0.012*\"forty\" + 0.012*\"demonstrate\" + 0.010*\"forfeit\" + 0.010*\"imperious\"\n",
      "31-Jan-2017 14:50:13 INFO gensim.models.ldamodel: topic #79 (0.010): 0.051*\"ample\" + 0.027*\"pack\" + 0.022*\"bruyère\" + 0.022*\"traced\" + 0.019*\"fallacies\" + 0.012*\"imperious\" + 0.012*\"forty\" + 0.012*\"step—we\" + 0.012*\"demonstrate\" + 0.011*\"forfeit\"\n",
      "31-Jan-2017 14:50:13 INFO gensim.models.ldamodel: topic #69 (0.010): 0.063*\"ample\" + 0.033*\"bruyère\" + 0.023*\"traced\" + 0.023*\"pack\" + 0.022*\"fallacies\" + 0.017*\"imperious\" + 0.015*\"step—we\" + 0.013*\"demonstrate\" + 0.012*\"forfeit\" + 0.011*\"forty\"\n",
      "31-Jan-2017 14:50:13 INFO gensim.models.ldamodel: topic diff=44.185255, rho=1.000000\n"
     ]
    }
   ],
   "source": [
    "#model = LdaModel(corpus=mm, id2word=type2id, num_topics=60, alpha = \"symmetric\", passes = 10) #import momentan in visual \n",
    "# -> da ich mir noch nicht sicher bin, welche Funktionen in das tm_gensim.py sollen\n",
    "model = LdaModel(corpus=mm, id2word=type2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ample',\n",
       " 'bruyère',\n",
       " 'pack',\n",
       " 'traced',\n",
       " 'fallacies',\n",
       " 'forfeit',\n",
       " 'demonstrate',\n",
       " 'forty',\n",
       " 'hypothetical',\n",
       " 'imperious']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_nr_x = model.get_topic_terms(10)\n",
    "\n",
    "[type2id[i[0]] for i in topic_nr_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = model.show_topics(num_topics = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13,\n",
       "  '0.066*\"ample\" + 0.033*\"pack\" + 0.030*\"bruyère\" + 0.023*\"traced\" + 0.021*\"fallacies\" + 0.018*\"imperious\" + 0.014*\"demonstrate\" + 0.011*\"forfeit\" + 0.010*\"hypothetical\" + 0.010*\"animation\"'),\n",
       " (37,\n",
       "  '0.041*\"ample\" + 0.021*\"pack\" + 0.020*\"bruyère\" + 0.016*\"imperious\" + 0.014*\"traced\" + 0.013*\"fallacies\" + 0.009*\"forfeit\" + 0.009*\"step—we\" + 0.009*\"forty\" + 0.008*\"demonstrate\"'),\n",
       " (58,\n",
       "  '0.045*\"ample\" + 0.028*\"pack\" + 0.028*\"bruyère\" + 0.024*\"traced\" + 0.020*\"fallacies\" + 0.017*\"imperious\" + 0.013*\"forty\" + 0.012*\"dishonour\" + 0.011*\"step—we\" + 0.010*\"artillery\"'),\n",
       " (22,\n",
       "  '0.053*\"ample\" + 0.030*\"pack\" + 0.026*\"bruyère\" + 0.024*\"traced\" + 0.019*\"demonstrate\" + 0.019*\"fallacies\" + 0.017*\"imperious\" + 0.015*\"forfeit\" + 0.014*\"step—we\" + 0.014*\"forty\"'),\n",
       " (3,\n",
       "  '0.058*\"ample\" + 0.034*\"bruyère\" + 0.029*\"pack\" + 0.023*\"traced\" + 0.019*\"imperious\" + 0.013*\"fallacies\" + 0.013*\"forfeit\" + 0.011*\"demonstrate\" + 0.011*\"forty\" + 0.010*\"step—we\"'),\n",
       " (95,\n",
       "  '0.043*\"ample\" + 0.032*\"bruyère\" + 0.022*\"demonstrate\" + 0.021*\"fallacies\" + 0.018*\"pack\" + 0.017*\"traced\" + 0.014*\"imperious\" + 0.014*\"dishonour\" + 0.012*\"step—we\" + 0.011*\"forfeit\"'),\n",
       " (33,\n",
       "  '0.063*\"ample\" + 0.031*\"bruyère\" + 0.029*\"pack\" + 0.024*\"traced\" + 0.019*\"fallacies\" + 0.018*\"imperious\" + 0.018*\"demonstrate\" + 0.016*\"forfeit\" + 0.014*\"forty\" + 0.011*\"step—we\"'),\n",
       " (99,\n",
       "  '0.046*\"ample\" + 0.025*\"pack\" + 0.025*\"bruyère\" + 0.022*\"traced\" + 0.022*\"fallacies\" + 0.015*\"imperious\" + 0.012*\"dishonour\" + 0.012*\"demonstrate\" + 0.012*\"forfeit\" + 0.012*\"forty\"'),\n",
       " (27,\n",
       "  '0.059*\"ample\" + 0.034*\"pack\" + 0.032*\"bruyère\" + 0.023*\"traced\" + 0.020*\"demonstrate\" + 0.019*\"fallacies\" + 0.014*\"imperious\" + 0.011*\"forty\" + 0.011*\"forfeit\" + 0.011*\"hypothetical\"'),\n",
       " (40,\n",
       "  '0.040*\"ample\" + 0.033*\"pack\" + 0.029*\"bruyère\" + 0.019*\"fallacies\" + 0.019*\"traced\" + 0.013*\"imperious\" + 0.013*\"demonstrate\" + 0.011*\"forty\" + 0.009*\"step—we\" + 0.009*\"forfeit\"'),\n",
       " (38,\n",
       "  '0.034*\"ample\" + 0.034*\"bruyère\" + 0.033*\"pack\" + 0.015*\"traced\" + 0.013*\"fallacies\" + 0.013*\"step—we\" + 0.012*\"forty\" + 0.012*\"demonstrate\" + 0.010*\"forfeit\" + 0.010*\"imperious\"'),\n",
       " (5,\n",
       "  '0.058*\"ample\" + 0.026*\"bruyère\" + 0.025*\"demonstrate\" + 0.024*\"pack\" + 0.023*\"traced\" + 0.022*\"fallacies\" + 0.014*\"imperious\" + 0.013*\"dishonour\" + 0.012*\"forty\" + 0.011*\"forfeit\"'),\n",
       " (64,\n",
       "  '0.055*\"ample\" + 0.032*\"pack\" + 0.027*\"bruyère\" + 0.022*\"traced\" + 0.022*\"fallacies\" + 0.017*\"imperious\" + 0.013*\"demonstrate\" + 0.012*\"forfeit\" + 0.012*\"forty\" + 0.011*\"dishonour\"'),\n",
       " (44,\n",
       "  '0.049*\"ample\" + 0.029*\"bruyère\" + 0.024*\"fallacies\" + 0.022*\"pack\" + 0.020*\"traced\" + 0.016*\"demonstrate\" + 0.014*\"forty\" + 0.012*\"imperious\" + 0.012*\"step—we\" + 0.011*\"forfeit\"'),\n",
       " (88,\n",
       "  '0.051*\"ample\" + 0.031*\"pack\" + 0.027*\"bruyère\" + 0.026*\"traced\" + 0.021*\"fallacies\" + 0.019*\"demonstrate\" + 0.016*\"forty\" + 0.016*\"imperious\" + 0.012*\"dishonour\" + 0.012*\"forfeit\"'),\n",
       " (78,\n",
       "  '0.047*\"ample\" + 0.032*\"pack\" + 0.029*\"traced\" + 0.020*\"bruyère\" + 0.020*\"fallacies\" + 0.019*\"forty\" + 0.017*\"imperious\" + 0.016*\"demonstrate\" + 0.012*\"step—we\" + 0.011*\"forfeit\"'),\n",
       " (42,\n",
       "  '0.055*\"ample\" + 0.029*\"bruyère\" + 0.024*\"fallacies\" + 0.022*\"pack\" + 0.019*\"forty\" + 0.019*\"traced\" + 0.013*\"step—we\" + 0.013*\"imperious\" + 0.012*\"forfeit\" + 0.012*\"hypothetical\"'),\n",
       " (75,\n",
       "  '0.072*\"ample\" + 0.029*\"pack\" + 0.028*\"bruyère\" + 0.021*\"fallacies\" + 0.019*\"traced\" + 0.016*\"imperious\" + 0.013*\"demonstrate\" + 0.011*\"forty\" + 0.011*\"artillery\" + 0.010*\"forfeit\"'),\n",
       " (57,\n",
       "  '0.034*\"ample\" + 0.023*\"bruyère\" + 0.017*\"pack\" + 0.016*\"imperious\" + 0.014*\"traced\" + 0.014*\"fallacies\" + 0.010*\"forfeit\" + 0.010*\"demonstrate\" + 0.009*\"step—we\" + 0.008*\"forty\"'),\n",
       " (87,\n",
       "  '0.057*\"ample\" + 0.031*\"pack\" + 0.021*\"bruyère\" + 0.016*\"fallacies\" + 0.016*\"imperious\" + 0.015*\"forty\" + 0.014*\"traced\" + 0.012*\"demonstrate\" + 0.011*\"forfeit\" + 0.010*\"animation\"'),\n",
       " (45,\n",
       "  '0.040*\"ample\" + 0.030*\"pack\" + 0.026*\"bruyère\" + 0.023*\"fallacies\" + 0.021*\"traced\" + 0.018*\"demonstrate\" + 0.018*\"imperious\" + 0.013*\"forty\" + 0.012*\"hypothetical\" + 0.011*\"step—we\"'),\n",
       " (56,\n",
       "  '0.034*\"ample\" + 0.026*\"pack\" + 0.024*\"bruyère\" + 0.013*\"fallacies\" + 0.013*\"traced\" + 0.011*\"imperious\" + 0.009*\"forfeit\" + 0.008*\"dishonour\" + 0.008*\"demonstrate\" + 0.007*\"forty\"'),\n",
       " (52,\n",
       "  '0.046*\"ample\" + 0.037*\"bruyère\" + 0.030*\"pack\" + 0.017*\"traced\" + 0.014*\"fallacies\" + 0.014*\"imperious\" + 0.014*\"demonstrate\" + 0.013*\"forty\" + 0.012*\"step—we\" + 0.010*\"forfeit\"'),\n",
       " (25,\n",
       "  '0.039*\"ample\" + 0.034*\"pack\" + 0.026*\"bruyère\" + 0.024*\"traced\" + 0.017*\"fallacies\" + 0.017*\"imperious\" + 0.014*\"demonstrate\" + 0.013*\"step—we\" + 0.013*\"forty\" + 0.012*\"dishonour\"'),\n",
       " (18,\n",
       "  '0.045*\"ample\" + 0.038*\"bruyère\" + 0.033*\"pack\" + 0.026*\"traced\" + 0.018*\"imperious\" + 0.014*\"fallacies\" + 0.013*\"forfeit\" + 0.013*\"demonstrate\" + 0.011*\"step—we\" + 0.010*\"horrible\"'),\n",
       " (89,\n",
       "  '0.063*\"ample\" + 0.024*\"bruyère\" + 0.023*\"pack\" + 0.022*\"fallacies\" + 0.019*\"traced\" + 0.016*\"demonstrate\" + 0.016*\"forty\" + 0.014*\"forfeit\" + 0.014*\"imperious\" + 0.012*\"hypothetical\"'),\n",
       " (51,\n",
       "  '0.065*\"ample\" + 0.032*\"pack\" + 0.030*\"bruyère\" + 0.024*\"traced\" + 0.019*\"fallacies\" + 0.017*\"imperious\" + 0.013*\"forfeit\" + 0.011*\"demonstrate\" + 0.010*\"dishonour\" + 0.009*\"step—we\"'),\n",
       " (0,\n",
       "  '0.040*\"ample\" + 0.040*\"pack\" + 0.020*\"bruyère\" + 0.019*\"fallacies\" + 0.015*\"traced\" + 0.014*\"forty\" + 0.013*\"demonstrate\" + 0.010*\"imperious\" + 0.010*\"step—we\" + 0.008*\"dishonour\"'),\n",
       " (50,\n",
       "  '0.046*\"ample\" + 0.040*\"bruyère\" + 0.030*\"pack\" + 0.019*\"traced\" + 0.017*\"imperious\" + 0.014*\"fallacies\" + 0.013*\"dishonour\" + 0.011*\"forfeit\" + 0.011*\"forty\" + 0.009*\"step—we\"'),\n",
       " (97,\n",
       "  '0.045*\"ample\" + 0.030*\"pack\" + 0.021*\"traced\" + 0.020*\"bruyère\" + 0.019*\"demonstrate\" + 0.016*\"forty\" + 0.014*\"fallacies\" + 0.011*\"hypothetical\" + 0.011*\"imperious\" + 0.010*\"dishonour\"'),\n",
       " (14,\n",
       "  '0.035*\"ample\" + 0.030*\"bruyère\" + 0.022*\"pack\" + 0.018*\"traced\" + 0.015*\"fallacies\" + 0.014*\"demonstrate\" + 0.012*\"forty\" + 0.011*\"forfeit\" + 0.010*\"imperious\" + 0.010*\"dishonour\"'),\n",
       " (83,\n",
       "  '0.052*\"ample\" + 0.025*\"pack\" + 0.022*\"bruyère\" + 0.021*\"traced\" + 0.015*\"imperious\" + 0.011*\"demonstrate\" + 0.010*\"forty\" + 0.010*\"fallacies\" + 0.009*\"artillery\" + 0.009*\"forfeit\"'),\n",
       " (9,\n",
       "  '0.057*\"ample\" + 0.024*\"bruyère\" + 0.021*\"pack\" + 0.020*\"traced\" + 0.020*\"fallacies\" + 0.015*\"imperious\" + 0.014*\"forty\" + 0.011*\"demonstrate\" + 0.010*\"forfeit\" + 0.009*\"artillery\"'),\n",
       " (77,\n",
       "  '0.042*\"ample\" + 0.030*\"pack\" + 0.028*\"bruyère\" + 0.024*\"traced\" + 0.018*\"demonstrate\" + 0.013*\"imperious\" + 0.013*\"forty\" + 0.012*\"fallacies\" + 0.011*\"step—we\" + 0.010*\"forfeit\"'),\n",
       " (65,\n",
       "  '0.052*\"ample\" + 0.033*\"pack\" + 0.021*\"bruyère\" + 0.018*\"fallacies\" + 0.017*\"demonstrate\" + 0.015*\"forty\" + 0.014*\"imperious\" + 0.013*\"traced\" + 0.012*\"hypothetical\" + 0.011*\"step—we\"'),\n",
       " (15,\n",
       "  '0.034*\"ample\" + 0.020*\"bruyère\" + 0.019*\"pack\" + 0.014*\"fallacies\" + 0.013*\"forty\" + 0.012*\"traced\" + 0.010*\"demonstrate\" + 0.009*\"imperious\" + 0.008*\"forfeit\" + 0.008*\"dishonour\"'),\n",
       " (86,\n",
       "  '0.034*\"ample\" + 0.023*\"pack\" + 0.022*\"bruyère\" + 0.019*\"traced\" + 0.015*\"fallacies\" + 0.011*\"demonstrate\" + 0.010*\"dishonour\" + 0.010*\"imperious\" + 0.010*\"forfeit\" + 0.009*\"step—we\"'),\n",
       " (59,\n",
       "  '0.046*\"ample\" + 0.027*\"pack\" + 0.022*\"bruyère\" + 0.019*\"traced\" + 0.018*\"demonstrate\" + 0.016*\"fallacies\" + 0.013*\"imperious\" + 0.012*\"forfeit\" + 0.012*\"forty\" + 0.011*\"step—we\"'),\n",
       " (32,\n",
       "  '0.049*\"ample\" + 0.027*\"pack\" + 0.025*\"bruyère\" + 0.021*\"traced\" + 0.017*\"fallacies\" + 0.017*\"demonstrate\" + 0.015*\"step—we\" + 0.013*\"forty\" + 0.012*\"imperious\" + 0.009*\"artillery\"'),\n",
       " (85,\n",
       "  '0.065*\"ample\" + 0.036*\"bruyère\" + 0.027*\"pack\" + 0.019*\"fallacies\" + 0.018*\"traced\" + 0.014*\"forfeit\" + 0.014*\"imperious\" + 0.010*\"dishonour\" + 0.010*\"step—we\" + 0.009*\"artillery\"'),\n",
       " (55,\n",
       "  '0.038*\"ample\" + 0.023*\"bruyère\" + 0.020*\"pack\" + 0.018*\"traced\" + 0.017*\"imperious\" + 0.016*\"fallacies\" + 0.015*\"demonstrate\" + 0.011*\"step—we\" + 0.009*\"forty\" + 0.009*\"hypothetical\"'),\n",
       " (2,\n",
       "  '0.057*\"ample\" + 0.030*\"pack\" + 0.022*\"bruyère\" + 0.018*\"fallacies\" + 0.017*\"traced\" + 0.015*\"imperious\" + 0.013*\"forty\" + 0.012*\"demonstrate\" + 0.011*\"forfeit\" + 0.011*\"step—we\"'),\n",
       " (74,\n",
       "  '0.049*\"ample\" + 0.029*\"bruyère\" + 0.025*\"pack\" + 0.019*\"traced\" + 0.019*\"fallacies\" + 0.016*\"imperious\" + 0.011*\"forty\" + 0.011*\"forfeit\" + 0.010*\"demonstrate\" + 0.010*\"step—we\"'),\n",
       " (48,\n",
       "  '0.038*\"ample\" + 0.032*\"pack\" + 0.030*\"traced\" + 0.024*\"bruyère\" + 0.020*\"fallacies\" + 0.016*\"demonstrate\" + 0.016*\"forty\" + 0.016*\"imperious\" + 0.015*\"step—we\" + 0.012*\"dishonour\"'),\n",
       " (54,\n",
       "  '0.053*\"ample\" + 0.031*\"pack\" + 0.031*\"bruyère\" + 0.019*\"traced\" + 0.018*\"fallacies\" + 0.016*\"demonstrate\" + 0.015*\"imperious\" + 0.013*\"forty\" + 0.013*\"step—we\" + 0.011*\"forfeit\"'),\n",
       " (1,\n",
       "  '0.059*\"ample\" + 0.031*\"pack\" + 0.030*\"bruyère\" + 0.019*\"traced\" + 0.018*\"fallacies\" + 0.013*\"forty\" + 0.013*\"demonstrate\" + 0.011*\"imperious\" + 0.010*\"dishonour\" + 0.010*\"hypothetical\"'),\n",
       " (93,\n",
       "  '0.080*\"ample\" + 0.040*\"bruyère\" + 0.039*\"pack\" + 0.023*\"fallacies\" + 0.019*\"traced\" + 0.014*\"imperious\" + 0.014*\"forfeit\" + 0.011*\"step—we\" + 0.011*\"demonstrate\" + 0.011*\"forty\"'),\n",
       " (7,\n",
       "  '0.057*\"ample\" + 0.033*\"pack\" + 0.027*\"bruyère\" + 0.023*\"traced\" + 0.020*\"imperious\" + 0.016*\"fallacies\" + 0.014*\"demonstrate\" + 0.013*\"forfeit\" + 0.011*\"step—we\" + 0.010*\"dishonour\"'),\n",
       " (41,\n",
       "  '0.040*\"ample\" + 0.031*\"bruyère\" + 0.021*\"pack\" + 0.021*\"traced\" + 0.018*\"fallacies\" + 0.018*\"demonstrate\" + 0.013*\"imperious\" + 0.013*\"forty\" + 0.012*\"dishonour\" + 0.010*\"step—we\"'),\n",
       " (81,\n",
       "  '0.045*\"ample\" + 0.028*\"pack\" + 0.027*\"bruyère\" + 0.025*\"fallacies\" + 0.024*\"traced\" + 0.016*\"demonstrate\" + 0.015*\"imperious\" + 0.013*\"forfeit\" + 0.013*\"step—we\" + 0.012*\"forty\"'),\n",
       " (68,\n",
       "  '0.060*\"ample\" + 0.029*\"bruyère\" + 0.027*\"pack\" + 0.021*\"traced\" + 0.020*\"fallacies\" + 0.017*\"imperious\" + 0.014*\"forty\" + 0.013*\"demonstrate\" + 0.011*\"step—we\" + 0.011*\"animation\"'),\n",
       " (61,\n",
       "  '0.063*\"ample\" + 0.039*\"bruyère\" + 0.028*\"traced\" + 0.027*\"pack\" + 0.018*\"fallacies\" + 0.016*\"forfeit\" + 0.014*\"demonstrate\" + 0.013*\"imperious\" + 0.011*\"step—we\" + 0.010*\"dishonour\"'),\n",
       " (20,\n",
       "  '0.046*\"ample\" + 0.027*\"pack\" + 0.024*\"traced\" + 0.021*\"fallacies\" + 0.020*\"bruyère\" + 0.014*\"forty\" + 0.014*\"imperious\" + 0.013*\"demonstrate\" + 0.010*\"step—we\" + 0.010*\"forfeit\"'),\n",
       " (17,\n",
       "  '0.054*\"ample\" + 0.032*\"bruyère\" + 0.031*\"traced\" + 0.028*\"pack\" + 0.017*\"fallacies\" + 0.016*\"demonstrate\" + 0.015*\"imperious\" + 0.012*\"forty\" + 0.010*\"forfeit\" + 0.010*\"dishonour\"'),\n",
       " (94,\n",
       "  '0.041*\"ample\" + 0.036*\"pack\" + 0.025*\"bruyère\" + 0.021*\"traced\" + 0.020*\"demonstrate\" + 0.017*\"fallacies\" + 0.015*\"imperious\" + 0.013*\"forfeit\" + 0.012*\"dishonour\" + 0.010*\"forty\"'),\n",
       " (47,\n",
       "  '0.044*\"ample\" + 0.031*\"bruyère\" + 0.029*\"pack\" + 0.021*\"traced\" + 0.017*\"fallacies\" + 0.016*\"demonstrate\" + 0.014*\"dishonour\" + 0.014*\"forty\" + 0.013*\"hypothetical\" + 0.012*\"imperious\"'),\n",
       " (80,\n",
       "  '0.043*\"ample\" + 0.029*\"bruyère\" + 0.027*\"pack\" + 0.020*\"fallacies\" + 0.018*\"traced\" + 0.015*\"forty\" + 0.015*\"dishonour\" + 0.015*\"demonstrate\" + 0.013*\"imperious\" + 0.013*\"forfeit\"'),\n",
       " (24,\n",
       "  '0.038*\"ample\" + 0.031*\"pack\" + 0.019*\"traced\" + 0.017*\"bruyère\" + 0.017*\"fallacies\" + 0.017*\"demonstrate\" + 0.014*\"forty\" + 0.012*\"step—we\" + 0.011*\"imperious\" + 0.010*\"forfeit\"'),\n",
       " (66,\n",
       "  '0.034*\"ample\" + 0.032*\"bruyère\" + 0.018*\"fallacies\" + 0.017*\"pack\" + 0.016*\"imperious\" + 0.014*\"traced\" + 0.013*\"dishonour\" + 0.012*\"demonstrate\" + 0.011*\"step—we\" + 0.009*\"forfeit\"'),\n",
       " (73,\n",
       "  '0.046*\"ample\" + 0.029*\"pack\" + 0.022*\"bruyère\" + 0.019*\"fallacies\" + 0.014*\"imperious\" + 0.013*\"traced\" + 0.013*\"forty\" + 0.012*\"demonstrate\" + 0.010*\"forfeit\" + 0.009*\"dishonour\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling mit Mallet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mallet Binary erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 15:42:36 DEBUG mallet: tutorial_supplementals/mallet_output/malletBinary.mallet\n",
      "31-Jan-2017 15:42:36 INFO mallet: Accessing Mallet ...\n",
      "31-Jan-2017 15:42:37 DEBUG mallet: Mallet file available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Variable zum Testen -> wenn Mallet unter Linux nicht installiert wurde\n",
    "path_to_mallet = \"/home/sina/Uni/Dariah/mallet/bin/mallet\"\n",
    "\n",
    "path_to_corpus = os.path.join(os.path.abspath('.'), 'corpus_txt')\n",
    "assert os.path.exists(path_to_corpus)\n",
    "\n",
    "#malletBinary = mal.create_mallet_binary(path_to_corpus)\n",
    "malletBinary = mal.create_mallet_binary(path_to_corpus, path_to_mallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mallet output erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-Jan-2017 15:42:40 DEBUG mallet: /home/sina/Uni/Dariah/DARIAH-Topics/Topics/tutorial_supplementals/mallet_output\n",
      "31-Jan-2017 15:42:40 INFO mallet: Accessing Mallet ...\n",
      "31-Jan-2017 15:42:40 DEBUG mallet: Mallet file available.\n"
     ]
    }
   ],
   "source": [
    "outfolder = os.path.join(os.path.abspath('.'), \"tutorial_supplementals/mallet_output\")\n",
    "mal.create_mallet_model(malletBinary, outfolder, path_to_mallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matrix von Mallet output erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basepath = os.path.join(os.path.abspath('.'), \"tutorial_supplementals/mallet_output\")\n",
    "doc_topics = os.path.join(basepath, \"doc_topics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.84700883e-02   1.22589081e-02   6.97940503e-02   8.33605754e-03\n",
      "    2.33736515e-02   3.80353057e-01   7.02844067e-03   8.87544949e-02\n",
      "    3.75939850e-03   5.83524027e-02   8.33605754e-03   3.77574371e-02\n",
      "    9.97057862e-03   8.17260543e-04   4.74011115e-03   1.78162798e-02\n",
      "    1.45635829e-01   9.26773455e-02   3.43249428e-03   8.33605754e-03]\n",
      " [  2.27659172e-02   2.05617482e-02   9.19138485e-02   1.10523333e-02\n",
      "    1.52087663e-02   1.58290824e-01   4.12494490e-03   9.82744505e-02\n",
      "    1.99319856e-02   7.82480005e-02   6.14018515e-03   2.69223503e-02\n",
      "    6.26613767e-03   1.98375213e-03   1.66887084e-03   2.09301593e-01\n",
      "    1.42924617e-01   7.52251401e-02   4.75470748e-03   4.43982619e-03]\n",
      " [  7.41623304e-03   1.90876818e-02   1.06866702e-01   2.84491563e-03\n",
      "    2.79409619e-01   1.15717551e-01   3.33122599e-03   8.66361912e-02\n",
      "    1.67047610e-02   5.32266693e-02   7.51349511e-03   2.39021544e-02\n",
      "    5.76277780e-03   5.59256918e-04   1.09419832e-03   1.33978505e-02\n",
      "    1.70621991e-01   7.99737392e-02   3.03943977e-03   2.89354666e-03]\n",
      " [  1.53721157e-02   2.64218395e-02   8.01104972e-02   2.17192070e-01\n",
      "    3.12967176e-02   1.38739032e-01   3.08742281e-03   8.27104322e-02\n",
      "    1.42021449e-02   7.99805005e-02   6.20734482e-03   2.21969451e-02\n",
      "    5.88235294e-03   1.72245694e-03   1.85245369e-03   1.67370816e-02\n",
      "    1.79948001e-01   6.81507962e-02   4.06239844e-03   4.12739682e-03]\n",
      " [  2.20680958e-03   5.36885246e-01   1.31462799e-01   4.09836066e-03\n",
      "    5.98991173e-03   2.20680958e-03   7.21941992e-02   1.29255990e-02\n",
      "    3.05800757e-02   5.39092055e-02   5.98991173e-03   6.52585120e-02\n",
      "    2.20680958e-03   5.35939470e-03   6.62042875e-03   7.25094578e-03\n",
      "    4.12988651e-02   6.62042875e-03   1.57629256e-03   5.35939470e-03]\n",
      " [  9.87193170e-03   1.07346140e-01   1.68356457e-01   5.60298826e-03\n",
      "    1.51191747e-03   1.14727855e-02   3.77534685e-01   1.69868374e-02\n",
      "    1.84098186e-02   7.94201352e-02   5.42511562e-03   1.12326574e-01\n",
      "    7.55958734e-03   2.93489861e-03   4.35787976e-03   5.06937033e-03\n",
      "    5.32728566e-02   1.11170402e-02   4.44681608e-04   9.78299538e-04]\n",
      " [  5.49450549e-03   2.37704918e-01   1.69789227e-01   3.51288056e-03\n",
      "    3.87317600e-03   8.37686903e-03   2.49954963e-01   1.84651414e-02\n",
      "    2.13475050e-02   7.05278328e-02   7.83642587e-03   1.08719150e-01\n",
      "    2.25184651e-03   9.63790308e-03   8.55701675e-03   3.33273284e-03\n",
      "    5.68366060e-02   1.01783462e-02   2.07169879e-03   1.53125563e-03]\n",
      " [  3.60576923e-03   1.35262574e-01   1.64108728e-01   2.31139053e-03\n",
      "    4.62278107e-04   1.54400888e-02   3.28309911e-01   2.98631657e-02\n",
      "    4.72448225e-02   5.59356509e-02   1.26664201e-02   1.00314349e-01\n",
      "    1.94156805e-03   6.74926036e-03   1.93232249e-02   1.38683432e-03\n",
      "    5.70451183e-02   1.48853550e-02   1.75665680e-03   1.38683432e-03]\n",
      " [  4.04976620e-01   2.35470942e-02   8.60053440e-02   7.51503006e-03\n",
      "    7.84903140e-03   4.25851703e-02   8.18303273e-03   4.99331997e-02\n",
      "    1.35270541e-02   1.22077488e-01   6.84702739e-03   2.28790915e-02\n",
      "    7.84903140e-03   1.61990648e-02   6.84702739e-03   3.84101536e-03\n",
      "    1.36773547e-01   2.68871075e-02   3.17301269e-03   2.50501002e-03]\n",
      " [  1.11689702e-02   5.88135098e-02   6.35828304e-02   2.11699435e-03\n",
      "    3.64999027e-04   1.08769710e-02   3.23632470e-03   9.90364026e-03\n",
      "    3.04165856e-03   2.32115048e-01   2.16566089e-03   5.81565116e-03\n",
      "    5.10998637e-04   4.19773214e-01   1.19233015e-03   1.67899552e-03\n",
      "    1.64857894e-01   5.08565310e-03   2.79832587e-03   9.00330932e-04]\n",
      " [  1.11578780e-02   2.09259848e-02   3.83973952e-02   3.05749682e-03\n",
      "    9.92693774e-04   2.71998094e-02   1.94567980e-03   2.57703304e-02\n",
      "    6.63119441e-03   1.62523825e-01   2.66041931e-03   8.14008895e-03\n",
      "    7.34593393e-03   6.23411690e-03   4.36785260e-04   1.94567980e-03\n",
      "    1.39413914e-01   1.63198856e-02   5.15208069e-01   3.69282084e-03]\n",
      " [  1.59848280e-03   9.56380385e-03   3.87157952e-02   9.48252506e-04\n",
      "    1.59848280e-03   4.52451910e-03   1.76104037e-03   1.14684367e-01\n",
      "    2.17583311e-01   1.74207532e-02   6.10945543e-02   6.20157139e-02\n",
      "    5.33730696e-03   1.11081008e-03   3.38797074e-01   7.31509076e-04\n",
      "    6.81387158e-02   4.61934435e-02   1.00243836e-03   7.17962612e-03]\n",
      " [  1.28627290e-02   2.41819304e-02   4.72319407e-02   8.54085203e-03\n",
      "    8.54085203e-03   4.45564931e-02   1.74933114e-03   7.13109693e-02\n",
      "    1.82239144e-01   2.70631817e-02   5.48466763e-02   1.03416341e-01\n",
      "    2.68470879e-01   1.95513480e-03   7.30603005e-03   9.98147767e-03\n",
      "    7.06935583e-02   4.47622968e-02   3.39576044e-03   6.89442272e-03]\n",
      " [  1.20440468e-03   1.68616655e-03   8.91259463e-03   1.06675843e-03\n",
      "    5.84996559e-04   1.54852030e-03   6.53819683e-04   3.95388851e-02\n",
      "    2.36407433e-02   3.20027529e-03   2.69270475e-01   6.22849277e-03\n",
      "    2.16792842e-03   5.16173434e-04   5.16173434e-04   1.20440468e-03\n",
      "    2.29525120e-02   2.03888507e-01   2.40880936e-04   4.10977288e-01]\n",
      " [  1.02813853e-02   2.65151515e-02   6.54761905e-02   1.35281385e-02\n",
      "    1.13636364e-02   3.95021645e-02   7.03463203e-03   5.35714286e-02\n",
      "    2.86796537e-02   6.22294372e-02   7.03463203e-03   7.62987013e-02\n",
      "    2.92748918e-01   8.11688312e-03   1.78571429e-02   2.32683983e-02\n",
      "    1.26082251e-01   1.10930736e-01   5.95238095e-03   1.35281385e-02]\n",
      " [  1.03652517e-02   7.15695953e-02   8.93385982e-02   2.46791708e-03\n",
      "    5.42941757e-03   6.36722606e-02   1.23395854e-02   2.22112537e-02\n",
      "    2.91214215e-02   1.92497532e-02   1.13524186e-02   4.34846989e-01\n",
      "    1.82625864e-02   5.42941757e-03   4.44225074e-03   7.40375123e-03\n",
      "    8.24284304e-02   6.66337611e-02   9.37808490e-03   3.40572557e-02]\n",
      " [  7.47986191e-03   3.64403529e-03   2.93440736e-02   5.56194860e-03\n",
      "    3.26045263e-03   9.49367089e-02   5.17836594e-03   1.21403913e-01\n",
      "    1.20828539e-02   2.81933257e-02   1.70694285e-02   1.74530111e-02\n",
      "    2.92481780e-01   2.87686997e-03   4.79478328e-03   5.56194860e-03\n",
      "    7.57575758e-02   1.91983122e-01   4.02761795e-03   7.69083237e-02]]\n"
     ]
    }
   ],
   "source": [
    "mal.create_MalletMatrix(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''lda_model = 'out_easy/corpus.lda'\n",
    "corpus = 'out_easy/corpus.mm'\n",
    "dictionary = 'out_easy/corpus.dict'\n",
    "doc_labels = 'out_easy/corpus_doclabels.txt'\n",
    "interactive  = False\n",
    "\n",
    "vis = visual.Visualization(lda_model, corpus, dictionary, doc_labels, interactive)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#heatmap = visual.make_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visual.save_heatmap(\"./visualizations/heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vis = collection.Visualization(lda_model, corpus, dictionary, doc_labels, interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vis.make_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vis.save_interactive(\"./visualizations/interactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ob ihr wirklich richtig steht, seht ihr, wenn ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![success](http://cdn2.hubspot.net/hub/128506/file-446943132-jpg/images/computer_woman_success.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
