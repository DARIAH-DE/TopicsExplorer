{% extends "base.html" %}

{% block main %}
<main class="main">
    <div class="main_content">
        <h1>A total of {{ number_topics }} topics in {{ corpus_size }} documents</h1>
        <p>Topics are probability distributions over the whole vocabulary of a text corpus. One value is
            assigned to each word, which indicates how <i>relevant</i> the word is to that topic (to be exact, how
            <i>likely</i> one word is to be found in a topic). After sorting those values in descending order, the first <i>n</i>
            words represent a topic.</p>
        <p>Below the topics are ranked by their <i>numerical dominance</i> in the corpus; each bar displays a topic’s dominance score.
            You can start exploring the topic model by <b>clicking on the topic bars</b> – or select one of the other tabs
            in the above menu.</p>
        {% for topic, proportion in proportions %}
        <p><a style="width: {{ proportion }}%;" class="main_button" href="{{ url_for('topics', topic=topic) }}">{{ topic }}</a></p>
        {% endfor %}
        <blockquote>Mathematically, the topic model has two goals in explaining the
            documents. First, it wants its topics to place high probability on few terms. Second, it wants to attach
            documents
            to as few topics as possible. These goals are at odds. With few terms assigned to each topic, the model
            captures the
            observed words by using more topics per article. With few topics assigned to each article, the model
            captures the
            observed words by using more terms per topic.<footer>
                <cite>
                    <a href="http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/">David M. Blei</a>
                </cite>
            </footer>
        </blockquote>
    </div>
</main>
{% endblock %}