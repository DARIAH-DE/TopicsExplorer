{% extends "base.html" %}

{% block main %}
<main class="main">
    <div class="main_content" id="upload_content" style="display: none;">
        <h1>Your corpus must be quite extensive...</h1>
        <p><b>Topics Explorer is fetching your text files at the moment</b>. This can take several seconds with large
            amounts of data. When
            everything is imported, you will automatically be redirected to a page with some more information about
            what’s going on.</p>
    </div>
    <div class="main_content" id="default_content">
        <form id="data" action="/modeling" method="POST" enctype="multipart/form-data">
            <h1>Explore your own text collection with a topic model – without prior knowledge.</h1>
            <p>The text mining technique <i>topic modeling</i> has become a popular procedure for clustering documents
                into semantic groups. This application introduces a user-friendly workflow which leads from raw text
                data to an interactive visualization of the topic model. All you need is a text corpus and a little
                time.</p>
            <blockquote>Topic modeling algorithms are statistical methods that analyze the words of the original texts
                to discover the themes that run through them, how those themes are connected to each other, and how
                they change over time.
                <footer>
                    <cite>
                        <a onclick="window.open('http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf', 'David M. Blei: Probabilistic Topic Models', 'location=yes,height=550,width=1120,scrollbars=yes,status=yes');"
                            href="#">David M. Blei</a>
                    </cite>
                </footer>
            </blockquote>
            <h2>1 Preprocessing</h2>
            <p>The corpus is tokenized first. This splits a text into individual words, so-called <i>tokens</i>. Token
                frequencies are typical units of analysis when working with text corpora. It may come as a surprise
                that reducing a book to a list of token frequencies retains useful information, but practice has shown
                this to be the case.</p>
            <blockquote>One assumption that topic models make is the bag of words assumption, that the order of the
                words in the document does not matter.<footer>
                    <cite>
                        <a onclick="window.open('http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf', 'David M. Blei: Probabilistic Topic Models', 'location=yes,height=550,width=1120,scrollbars=yes,status=yes');"
                            href="#">David M. Blei</a>
                    </cite>
                </footer>
            </blockquote>
            <p>You can select any plain text files – markup will be stripped. Check out <a onclick="window.open('https://textgrid.de/en/digitale-bibliothek', 'TextGrid', 'location=yes,height=550,width=1120,scrollbars=yes,status=yes');"
                            href="#">TextGrid</a> for an extensive collection of German texts.</p>
            <p><input type="file" name="corpus" accept=".txt, .xml, .html" multiple required /></p>
            <p>The frequency distribution of words in a text corpus follows <a onclick="window.open('https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176592/', 'Steven T. Piantadosi: Zipf’s word frequency law in natural language', 'location=yes,height=550,width=1120,scrollbars=yes,status=yes');"
                    href="#">Zipf’s
                    law</a>, which implies that <i>few types</i> occur <i>very frequently</i>, and <i>many types</i>
                occur <i>very
                    rarely</i>. In topic modeling, we are only interested in words in the middle frequency range; the
                most common words are usually empty function words, and the rarest words so specific that they
                are of no use to the model.</p>
            <p>You can either set a threshold for the most common words to remove:</p>
            <p><input type="number" name="mfw" value="100" min="1"></p>
            <p>or select an external list of words to be removed (which is recommended):</p>
            <p><input type="file" name="stopwords"></p>
            <h2>2 Modeling</h2>
            <p>A parameter is any characteristic that can help in defining or classifying a particular system – the
                topic model. You will have to adjust two model parameters: the number of topics, i.e. how <i>many</i>
                semantic clusters should be formed, and the number of iterations, i.e. how <i>long</i> the model should
                learn from the data.</p>
            <blockquote>Latent Dirichlet allocation, a generative probabilistic topic model, is a three-level
                hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an
                underlying set of topics.<footer>
                    <cite>
                        <a onclick="window.open('http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf', 'David M. Blei, Andrew Y. Ng, Michale I. Jordan: Latent Dirichlet Allocation', 'location=yes,height=550,width=1120,scrollbars=yes,status=yes');"
                            href="#">David M. Blei, Andrew Y. Ng,
                            Michale I. Jordan</a>
                    </cite>
                </footer>
            </blockquote>
            <p>The ideal number of topics depends on what you are looking for in the model. The default value gives
                a broad overview of your text collection’s contents:</p>
            <p><input type="number" name="topics" value="10" min="1" required></p>
            <p>The number of sampling iterations should be a trade-off between the time taken to complete sampling and
                the quality of the model:</p>
            <p><input type="number" name="iterations" value="100" min="10" required></p>
            <h2>3 Visualizing</h2>
            <p>When using topic models to explore text collections, one is typically interested in examining texts in
                terms of their constituent topics – instead of pure word frequencies. Because the number of topics is
                so much smaller than the number of unique vocabulary elements (say, 10 versus 10,000), a range of data
                visualization methods become available.</p>
            <p>You will be able to navigate through topics and documents, get similar topics and documents displayed,
                read
                excerpts from the original texts, and inspect the <i>document-topic distributions</i> in a heatmap.</p>
            <blockquote>Topic models are high-level statistical tools. A user must scrutinize numerical distributions
                to understand and explore their results; the raw output of the model is not enough to create an easily
                explored corpus.<footer>
                    <cite>
                        <a onclick="window.open('https://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/viewFile/4645/5021', 'Allison J. B. Chaney, David M. Blei: Visualizing Topic Models', 'location=yes,height=550,width=1120,scrollbars=yes,status=yes');"
                            href="#">Allison J. B. Chaney, David M. Blei</a>
                    </cite>
                </footer>
            </blockquote>
            <p><button type="submit">Train Topic Model</button></p>
        </form>
    </div>
</main>
<script>
    $('#data').submit(function (event) {
        $('#default_content').css({
            'display': "none"
        });
        $('#upload_content').css({
            'display': 'inline'
        });
    });
</script>
{% endblock %}